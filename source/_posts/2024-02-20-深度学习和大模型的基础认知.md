---
title: 深度学习和大模型的基础认知
date: 2024-02-20
categories: 
- llm
tags:
- 大模型部署
- ai
- base
------

在当今技术领域，大家谈论的时候常常涉及到“大模型”，对于很多新手来说，这个概念可能还比较模糊。为了让大家更好地参与讨论，让我们一起来了解一些关于深度学习和大模型的基础知识。

## 什么是机器学习？

机器学习简而言之，就是机器通过学习数据，自动找到一个函数。这个函数的作用是，给定输入，输出对应的预测结果。在数学上，这个函数可以表示为 `y = f(x)`，其中 `x` 是输入，`y` 是输出。比如，我们可以通过一个函数来预测摄入食物的卡路里，如 `卡路里 = 200 * 面包数量`。在机器学习领域，这个函数可能是一个包含上千亿参数的巨大函数，只有机器能够处理和理解。

## 模型

模型（model）就是一个函数，而大模型则是一个巨大的函数。机器学习根据函数输出进行分类，包括回归、分类和生成式学习。

* 回归（Regression）：函数的输出是一个数值，例如通过今天的 PM2.5、温度来预测明天的 PM2.5。
* 分类（Classification）：函数的输出是一个类别，比如判断一封邮件是否为垃圾邮件。
* 生成式学习（Generative Learning）：生成结构化文件，如影像、语音或文句。

## 什么是深度学习？

深度学习是机器学习的一种，通过深层神经网络学习复杂的表示。以预测YouTube频道观看人数为例，最初的做法是使用线性函数 ```Y=b+wX```。通过定义损失函数，并利用梯度下降方法，寻找最优参数 b 和 w 以使损失最小化。

### 最原始做法

```python
# 定义损失函数
def loss_function(Y, predicted_Y):
    return (Y - predicted_Y)**2

# 初始化参数
b = 0
w = 0

# 学习率
learning_rate = 0.01

# 迭代次数
epochs = 1000

# 梯度下降迭代
for epoch in range(epochs):
    # 计算梯度
    gradient_b = -2 * (Y - (b + w * X))
    gradient_w = -2 * X * (Y - (b + w * X))
    
    # 更新参数
    b = b - learning_rate * gradient_b
    w = w - learning_rate * gradient_w

# 训练后的参数
print("b:", b)
print("w:", w)

```
但在测试数据上，这种简单线性模型的表现可能不佳。

### 多看几天的数据
```
# 使用前7天的数据
X = [day1, day2, day3, day4, day5, day6, day7]

# 其他步骤同上


```
通过引入更多天数的数据，如前 7 天的数据，我们得到更复杂的模型。逐渐增加参数量，可以提高预测效果，但存在过拟合的风险。


### 线性模型太简单
线性函数模型（Linear Model 比如 Y=b+wX）因为是直线，非常简单。不能模拟真实情况，真实的情况往往都不是简单线性关系，我们可以通过叠加多个sigmoid函数来达成。

```
# 使用多个 sigmoid 函数叠加
def complex_model(X, parameters):
    result = 0
    for i in range(len(parameters)):
        result += parameters[i] * sigmoid(X[i])
    return result

# 其他步骤同上
```

### 深度学习

深度学习使用多个神经元(sigmoid函数)叠加的方式，构建更复杂的函数模型。神经网络中的每个 sigmoid 函数即为神经元，多层次的叠加构成深度学习模型。深度学习是对神经网络的发展，而不是直接模拟人类大脑。

在深度学习训练时，需要调整一些超参数：

* Batch size：对训练数据进行分批处理。
* 学习率：用于梯度下降，影响模型训练速度和精度。
* 神经元数量：影响函数复杂度，越多越复杂。
* 隐藏层数：层数越深，函数越复杂。

深度学习的参数数量取决于神经元数量和隐藏层数，而大模型的参数量可能达到千亿级别。

## 什么是LLM大语言模型？
大语言模型（LLM）是基于海量文本数据训练的深度学习模型。它不仅能生成自然语言文本，还能深入理解文本含义，处理各种自然语言任务。参数量不断提升，使得模型能更细致地捕捉语言微妙之处。


## 为什么大模型需要大量计算资源？

大模型的计算量可通过 FLOPS 进行估算，通常使用 GPU 进行训练。以 GPT-3 为例，其总算力需求与模型参数量、词数和单词运算量有关。对于训练和推理，计算量需求巨大，需要强大的计算资源，如数千张 GPU 卡。。

### 跑一下大模型，感受下计算量

通过示例代码，我们可以在本地实践一个大模型的运行，这里以 Microsoft 开源的 Phi-2 Transformer 模型为例。该模型有 27 亿参数，使用 96 张 A100-80G 的显卡训练 14 天完成。代码演示了加载模型和进行推理的过程。

```
# 示例代码（以Phi-2 Transformer为例）
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# 加载预训练模型和分词器
model = GPT2LMHeadModel.from_pretrained("microsoft/philosopher-2.0-large")
tokenizer = GPT2Tokenizer.from_pretrained("microsoft/philosopher-2.0-large")

# 输入文本
input_text = "深度学习是"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# 生成文本
output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)

# 解码并打印生成的文本
decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)
print(decoded_output)

```

---
参考：

* 关于深度学习和大模型的基础认知： https://mp.weixin.qq.com/s/Q0746IMKNiX0_5EXAPsNKA
* 李宏毅深度学习课程：https://www.youtube.com/watch?v=Ye018rCVvOo
* 梯度下降：https://www.jianshu.com/p/c7e642877b0e
* phi-2模型：https://huggingface.co/microsoft/phi-2


